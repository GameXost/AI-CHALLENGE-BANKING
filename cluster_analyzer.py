import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import umap

try:
	import hdbscan

	HDBSCAN_AVAILABLE = True
except ImportError:
	HDBSCAN_AVAILABLE = False
	print("‚ö†Ô∏è HDBSCAN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install hdbscan")
	print("   –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω DBSCAN –∫–∞–∫ fallback")


class AutoClusterAnalyzer:
	"""
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HDBSCAN

    –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê –ü–ï–†–ï–î DBSCAN:
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    - –ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–≥–æ –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ eps/min_samples
    - –ù–∞—Ö–æ–¥–∏—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã
    - –ü–æ–º–µ—á–∞–µ—Ç –≤—ã–±—Ä–æ—Å—ã –∫–∞–∫ —à—É–º

    –ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
    1. StandardScaler - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    2. UMAP - —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 104 ‚Üí 10-15 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
    3. HDBSCAN - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    4. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è - —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–µ–π –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    """

	def __init__(self):
		self.scaler = StandardScaler()
		self.umap_reducer = None
		self.clusterer = None
		self.cluster_profiles = {}

	def train_clustering_model(self, feature_matrix, info_df, feature_columns):
		"""
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            feature_matrix: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            info_df: DataFrame —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è—Ö
            feature_columns: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
		print(f"\nüî¨ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø (HDBSCAN + UMAP)")
		print("-" * 70)

		if len(feature_matrix) < 30:
			print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (<30 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)")
			info_df['ml_cluster'] = 0
			return info_df

		# 1. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
		X = feature_matrix[feature_columns].fillna(0)
		X_scaled = self.scaler.fit_transform(X)

		print(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
		print(f"   –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(X)}")
		print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_columns)}")

		# 2. –°–ù–ò–ñ–ï–ù–ò–ï –†–ê–ó–ú–ï–†–ù–û–°–¢–ò –° UMAP
		print(f"\nüß¨ –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (UMAP)...")
		n_components = min(15, X_scaled.shape[1] - 1)

		self.umap_reducer = umap.UMAP(
			n_components=n_components,
			n_neighbors=15,
			min_dist=0.0,  # –î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ª—É—á—à–µ 0.0
			metric='euclidean',
			random_state=42
		)
		X_reduced = self.umap_reducer.fit_transform(X_scaled)
		print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {X_scaled.shape[1]} ‚Üí {X_reduced.shape[1]}")

		# 3. –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø –° HDBSCAN
		if not HDBSCAN_AVAILABLE:
			print("\n‚ùå HDBSCAN –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω! –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è DBSCAN...")
			from sklearn.cluster import DBSCAN
			dbscan = DBSCAN(eps=0.5, min_samples=10)
			cluster_labels = dbscan.fit_predict(X_reduced)
		else:
			print(f"\nüéØ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (HDBSCAN)...")

			# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç min_cluster_size
			# –ü—Ä–∞–≤–∏–ª–æ: 2-5% –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–ª–∏ –º–∏–Ω–∏–º—É–º 10
			min_cluster_size = max(10, int(len(X_reduced) * 0.03))
			min_samples = max(5, int(min_cluster_size * 0.5))

			self.clusterer = hdbscan.HDBSCAN(
				min_cluster_size=min_cluster_size,
				min_samples=min_samples,
				metric='euclidean',
				cluster_selection_method='eom',  # Excess of Mass
				prediction_data=True
			)

			cluster_labels = self.clusterer.fit_predict(X_reduced)

			print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
			print(f"      min_cluster_size: {min_cluster_size}")
			print(f"      min_samples: {min_samples}")

		# 4. –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
		unique_labels = np.unique(cluster_labels)
		n_clusters = len(unique_labels[unique_labels != -1])
		noise_points = np.sum(cluster_labels == -1)
		noise_ratio = noise_points / len(cluster_labels)

		print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò:")
		print(f"   –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
		print(f"   –®—É–º (–≤—ã–±—Ä–æ—Å—ã): {noise_points} ({noise_ratio:.1%})")

		# 5. –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê
		if n_clusters > 1:
			mask = cluster_labels != -1
			if np.sum(mask) > n_clusters:
				silhouette = silhouette_score(X_reduced[mask], cluster_labels[mask])
				davies_bouldin = davies_bouldin_score(X_reduced[mask], cluster_labels[mask])
				calinski = calinski_harabasz_score(X_reduced[mask], cluster_labels[mask])

				print(f"\nüéØ –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê:")
				print(f"   Silhouette Score: {silhouette:.3f} (–≤—ã—à–µ = –ª—É—á—à–µ, 0.3-0.7 —Ö–æ—Ä–æ—à–æ)")
				print(f"   Davies-Bouldin Index: {davies_bouldin:.3f} (–Ω–∏–∂–µ = –ª—É—á—à–µ)")
				print(f"   Calinski-Harabasz Score: {calinski:.1f} (–≤—ã—à–µ = –ª—É—á—à–µ)")

		# 6. –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú
		print(f"\nüìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô:")
		for cluster_id in sorted(unique_labels):
			if cluster_id == -1:
				continue
			cluster_size = np.sum(cluster_labels == cluster_id)
			percentage = (cluster_size / len(cluster_labels)) * 100
			print(f"   –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {cluster_size} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π ({percentage:.1f}%)")

		if noise_points > 0:
			print(f"   –®—É–º: {noise_points} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π ({noise_ratio:.1%})")

		# 7. –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í
		self.cluster_profiles = self._interpret_clusters(
			X_scaled, feature_matrix, cluster_labels, feature_columns
		)

		# 8. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
		info_df['ml_cluster'] = cluster_labels

		return info_df

	def _interpret_clusters(self, X_scaled, feature_matrix, cluster_labels, feature_columns):
		"""
        –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

        –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å:
        - –¢–æ–ø-5 –æ—Ç–ª–∏—á–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        - –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        - –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        """
		print(f"\nüîç –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –ö–õ–ê–°–¢–ï–†–û–í:")
		print("-" * 70)

		cluster_profiles = {}
		unique_clusters = np.unique(cluster_labels)

		# –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
		key_features = [
			'age_numeric', 'socdem_cluster', 'engagement_level',
			'financial_activity', 'impulse_score', 'search_ratio',
			'mean_price', 'payment_count', 'total_actions',
			'weekend_ratio', 'product_diversity', 'credit_products_affinity',
			'saving_products_affinity', 'spending_trend'
		]

		# –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
		available_key_features = [f for f in key_features if f in feature_columns]

		for cluster_id in unique_clusters:
			if cluster_id == -1:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —à—É–º
				continue

			cluster_mask = cluster_labels == cluster_id
			cluster_size = np.sum(cluster_mask)

			if cluster_size < 3:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –∫–ª–∞—Å—Ç–µ—Ä
				continue

			# –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
			cluster_data = feature_matrix[feature_columns].iloc[cluster_mask]

			# –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
			cluster_means = {}
			for feature in available_key_features:
				cluster_means[feature] = cluster_data[feature].mean()

			# –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-5 –æ—Ç–ª–∏—á–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
			# (–ø—Ä–∏–∑–Ω–∞–∫–∏, –≥–¥–µ —Å—Ä–µ–¥–Ω–µ–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –æ–±—â–µ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ)
			all_means = feature_matrix[feature_columns].mean()
			feature_importance = {}

			for feature in feature_columns:
				if feature_matrix[feature].std() > 0:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
					cluster_mean = cluster_data[feature].mean()
					overall_mean = all_means[feature]
					overall_std = feature_matrix[feature].std()

					# Z-score —Ä–∞–∑–ª–∏—á–∏—è
					z_score = abs(cluster_mean - overall_mean) / overall_std
					feature_importance[feature] = z_score

			# –¢–æ–ø-5 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
			top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:5]

			# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
			cluster_type = self._classify_cluster_type(cluster_means)

			# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ—Ñ–∏–ª—å
			cluster_profiles[cluster_id] = {
				'size': cluster_size,
				'type': cluster_type,
				'key_metrics': cluster_means,
				'top_features': top_features,
			}

			# –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
			print(f"\nüìã –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} ({cluster_size} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π) - '{cluster_type}'")
			print(f"   –¢–æ–ø-5 –æ—Ç–ª–∏—á–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
			for feature, importance in top_features:
				value = cluster_data[feature].mean()
				print(f"      ‚Ä¢ {feature}: {value:.3f} (–≤–∞–∂–Ω–æ—Å—Ç—å: {importance:.2f})")

		return cluster_profiles

	def _classify_cluster_type(self, cluster_means):
		"""
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ:
        - "–ú–æ–ª–æ–¥—ã–µ –∞–∫—Ç–∏–≤–Ω—ã–µ"
        - "–ü—Ä–µ–º–∏—É–º-–∫–ª–∏–µ–Ω—Ç—ã"
        - "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏"
        - "–ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ"
        - –∏ —Ç.–¥.
        """
		# –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
		age = cluster_means.get('age_numeric', 3)
		financial = cluster_means.get('financial_activity', 0)
		engagement = cluster_means.get('engagement_level', 0)
		impulse = cluster_means.get('impulse_score', 0)
		search = cluster_means.get('search_ratio', 0)
		mean_price = cluster_means.get('mean_price', 0)
		credit_affinity = cluster_means.get('credit_products_affinity', 0)
		saving_affinity = cluster_means.get('saving_products_affinity', 0)

		# –õ–æ–≥–∏–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
		if mean_price > 6000 and financial > 0.5:
			return "–ü—Ä–µ–º–∏—É–º-–∫–ª–∏–µ–Ω—Ç—ã"
		elif age < 2 and engagement > 0.4:
			return "–ú–æ–ª–æ–¥—ã–µ –∞–∫—Ç–∏–≤–Ω—ã–µ"
		elif search > 0.5 and impulse < 0.3:
			return "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏"
		elif impulse > 0.6:
			return "–ò–º–ø—É–ª—å—Å–∏–≤–Ω—ã–µ –ø–æ–∫—É–ø–∞—Ç–µ–ª–∏"
		elif credit_affinity > 0.3:
			return "–ö—Ä–µ–¥–∏—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ"
		elif saving_affinity > 0.3:
			return "–ù–∞–∫–æ–ø–∏—Ç–µ–ª–∏"
		elif engagement < 0.2 and financial < 0.2:
			return "–ù–µ–∞–∫—Ç–∏–≤–Ω—ã–µ"
		elif age > 3.5:
			return "–ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ"
		else:
			return "–°—Ä–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç"

	def get_cluster_profile(self, cluster_id):
		"""–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
		return self.cluster_profiles.get(cluster_id, None)

	def predict_cluster(self, new_user_features):
		"""
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å HDBSCAN —Å prediction_data=True
        """
		if self.clusterer is None or not HDBSCAN_AVAILABLE:
			raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ –∏–ª–∏ HDBSCAN –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

		# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
		X_scaled = self.scaler.transform(new_user_features.reshape(1, -1))
		X_reduced = self.umap_reducer.transform(X_scaled)

		# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
		cluster_label, strength = hdbscan.approximate_predict(self.clusterer, X_reduced)

		return cluster_label[0], strength[0]